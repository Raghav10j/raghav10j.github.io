---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<p>(Last Updated: Oct 30, 2023)</p>

<p> I am an incoming research associate at the National Centre for Text Mining at the University of Manchester, where I work under Dr. <a href="https://scholar.google.com/citations?user=quhi-K0AAAAJ&hl=en">Sophia Ananiadou</a> exploring the scientific document understanding capabilities of large language models (LLMs).</p>

<p> I also currently collaborate with the <a href="http://www.robots.ox.ac.uk/~tvg/">Oxford TVG Lab</a> (Prof. Philip Torr)and Meta researchers (<a href="https://scholar.google.com/citations?user=HX0BfLYAAAAJ&hl=en">Ser Nam Lim</a>) on identifying and mitigating the potential for influence operations on social media using LLMs.</p>

<p>My past research experience includes working with the joint NLP Lab of IIT Patna and IIT Bombay under the guidance of <a href="https://www.iitp.ac.in/~sriparna/">Dr. Sriparna Saha</a> and <a href="https://scholar.google.co.in/citations?hl=en&user=vvg-pAkAAAAJ&view_op=list_works&sortby=pubdate">Dr. Pushpak Bhattacharyya</a>. There, I work on a variety of NLP problems and collaborate with researchers from Microsoft, the University of Tokyo, and Amazon Alexa. More broadly, I worked on online safety, AI integrity, and user analytics. This encompasses projects such as developing advanced AI-powered content moderation techniques, testing the boundaries of LLMs, and implementing AI across sectors including education, law, and healthcare. My work has led to numerous publications in top-tier venues including EMNLP 2023, ACL 2023, CIKM 2023, ECML 2023, ACM MM 2023, ECAI 2023, ECIR 2022, and IJCNN 2022/2023.
</p>

<p>I received my undergraduate degree from <a href="http://www.dtu.ac.in/">Delhi Technological University</a> in Software Engineering in 2021.</p>

<p><bold>My research focuses on developing techniques to ensure generative AI systems are safe, ethical, and socially responsible as their capabilities continue to rapidly advance. I am particularly interested in studying and mitigating the potential societal impacts of increasingly powerful generative models, with specific emphasis on the following aspects</bold>:
<li>Investigating techniques to detect and mitigate potential misuse of generative AI for harmful purposes like spreading misinformation, toxicity, and influence operations on social media. Exploring best practices to develop generative models resilient to malicious applications.</li>
<li>Analyzing sources of social bias in generative models, including assessing impacts of training data. Researching methods to measure, understand, and reduce inherent biases in order to make generative systems more fair and inclusive. Evaluating approaches for enabling models to "forget" or override harmful biases.</li>
<li>Advancing understanding of large language model training dynamics and interpretability. Studying how linguistic relationships emerge and features interact during the training process.</li><p>




<p>I am also a core member of <a href="https://simppl.org/">SimPPL</a>, a research collective in the global south, developing open-access trust and safety tools for international orgs. including nonprofits and newsrooms. We aim to build better civic integrity tools such as <a href="https://parrot.report/">Parrot Report</a>. We’ve worked with Anti Defamation League, <a href="https://techglobalinstitute.com/">Tech Global Institute</a>, The Sunday Times, Ippen Media, Deutsche Welle, and others on:
<li>Study the spread of unreliable news and identify influence operations online (<a href="https://parrot.report/">Parrot Report</a>).</li>
<li>Deploying a Whatsapp-based chatbot that engages in menstrual health discussions over a fixed dataset of documents encapsulating local knowledge, societal norms, domain-knowledge obtained from local NGO partners (<a href="https://sakhi.simppl.org/en">Sakhi Bot</a>)</li>
</p> 

<div style="float:left; width:100%; overflow-y: auto; height: 400px;">
<ul>

<li>Oct 2023- Two papers were accepted into NeurIPS workshops: one on Language Model interpretability for the <a href="https://attrib-workshop.cc/">Attributing Model Behavior at Scale workshop</a>, and a review paper on social media simulation for the <a href="http://masec.ai/">Multi-Agent Security Workshop.</a></li>

<li>Oct 2023- Our AACL 23 paper, "Reimagining Complaint Analysis: Adopting Seq2Path for a Generative Text-to-Text Framework" has been selected for the Area Chair Award(Information Extraction) </li>

<li>Oct 2023- Three of my papers have been accepted for the EMNLP '23 conference (Core A*): two for the main conference and one for the findings section. Preprint coming soon.</li>

<li>Sept 2023- One paper titled "Reimagining Complaint Analysis: Adopting Seq2Path for a Generative Text-to-Text Framework" has been accepted for the AACL 23 conference. Preprint coming soon.</li>

<li>Aug 2023- One paper titled <a href="https://dl.acm.org/doi/10.1145/3583780.3614937">"Investigating the Impact of Multimodality and External Knowledge in Aspect-level Complaint and Sentiment Analysis"</a> has been accepted for CIKM 23 conference (A ranking conference).</li>

<li>Jul 2023- Our paper titled <a href="https://dl.acm.org/doi/10.1145/3581783.3613776">"AbCoRD: Exploiting multimodal generative approach for Aspect-based Complaint and Rationale Detection"</a> has been accepted for publication in ACM MM 2023, a Core A* conference.</li>

<li>Jul 2023- Our paper titled "<a href="https://www.researchgate.net/publication/374324622_T-VAKS_A_Tutoring-Based_Multimodal_Dialog_System_via_Knowledge_Selection">T-VAKS: A Tutoring-Based Multimodal Dialog System via Knowledge Selection</a>" has been accepted for publication in ECAI 2023, a Core A conference. </li>

<li>Jul 2023- Our paper titled "<a href="https://www.cse.iitb.ac.in/~pb/papers/nle23-stereohate.pdf">StereoHate: Towards identifying Stereotypical Bias and Target group in Hate Speech Detection</a>" has been accepted into Natural Language Engineering Journal.</li>

<li>June 2023- One paper titled <a href="https://www.springerprofessional.de/aspect-based-complaint-and-cause-detection-a-multimodal-generati/26052040">Aspect-based Complaint and Cause Detection: A Multimodal Generative Framework with External Knowledge Infusion</a> has been accepted in ECML 23 conference (Core A) </li>
<li>May 2023- One paper titled <a href="https://aclanthology.org/2023.acl-long.404/">Peeking inside the black box: A Commonsense-aware Generative Framework for Explainable Complaint Detection</a> has been accepted into ACL 23 main conference (Core A*).</li>
<li>April 2023- One paper titled <a href="https://link.springer.com/chapter/10.1007/978-3-031-41682-8_9">“Explain Thyself Bully”: Sentiment Aided Cyberbullying Detection with Explanation</a> has been accepted into ICDAR 23 conference (Core A)</li>
</ul>
</div>
